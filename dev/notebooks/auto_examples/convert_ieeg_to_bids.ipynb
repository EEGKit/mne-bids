{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n.. currentmodule:: mne_bids\n\n# 08. Convert iEEG data to BIDS format\n\nIn this example, we use MNE-BIDS to create a BIDS-compatible directory of iEEG\ndata. Specifically, we will follow these steps:\n\n1. Download some iEEG data.\n\n2. Load the data, extract information, and save in a new BIDS directory.\n\n3. Check the result and compare it with the standard.\n\n4. Cite MNE-BIDS.\n\n5. Repeat the process for the ``fsaverage`` template coordinate space.\n\nThe iEEG data will be written by :func:`write_raw_bids` with\nthe addition of extra metadata elements in the following files:\n\n- the sidecar file ``ieeg.json``\n- ``electrodes.tsv``\n- ``coordsystem.json``\n- ``events.tsv``\n- ``channels.tsv``\n\nCompared to EEG data, the main differences are within the\n``coordsystem.json`` and ``electrodes.tsv`` files.\nFor more information on these files,\nrefer to the `iEEG part of the BIDS specification`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Adam Li <adam2392@gmail.com>\n#          Stefan Appelhoff <stefan.appelhoff@mailbox.org>\n#          Alex Rockhill <aprockhill@mailbox.org>\n#\n# License: BSD-3-Clause"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os.path as op\nimport numpy as np\nimport shutil\n\nimport nibabel as nib\nfrom nilearn.plotting import plot_anat\n\nimport mne\nfrom mne_bids import (BIDSPath, write_raw_bids, write_anat,\n                      get_anat_landmarks, read_raw_bids,\n                      search_folder_for_text, print_dir_tree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Download the data\n\nFirst, we need some data to work with. We will use the\ndata downloaded via MNE-Python's ``datasets`` API:\n:func:`mne.datasets.misc.data_path`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "misc_path = mne.datasets.misc.data_path()\n\n# The electrode coords data are in the tsv file format\n# which is easily read in using numpy\nraw = mne.io.read_raw_fif(op.join(\n    misc_path, 'seeg', 'sample_seeg_ieeg.fif'))\nraw.info['line_freq'] = 60  # specify power line frequency as required by BIDS\nsubjects_dir = op.join(misc_path, 'seeg')  # Freesurfer recon-all directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the locations of the channels in this dataset were found\nin `Locating Intracranial Electrode Contacts\n<https://mne.tools/dev/auto_tutorials/clinical/10_ieeg_localize.html>`_,\nthe T1 was aligned to ACPC. So, this montage is in an\n`ACPC-aligned coordinate system\n<https://surfer.nmr.mgh.harvard.edu/fswiki/CoordinateSystems>`_.\nWe can either save the channel positions in the subject's anatomical\nspace (from their T1 image) or we can transform to a template space\nsuch as ``fsaverage``. To save them in the individual space, it is\nrequired that the T1 have been aligned to ACPC and then the channel positions\nbe in terms of that coordinate system. Automated alignment to ACPC has not\nbeen implemented in MNE yet, so if the channel positions are not in\nan ACPC-aligned coordinate system, using a template (like ``fsaverage``)\nis the best option.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# estimate the transformation from \"head\" to \"mri\" space\ntrans = mne.coreg.estimate_head_mri_t('sample_seeg', subjects_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's convert the montage to \"mri\"\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "montage = raw.get_montage()\nmontage.apply_trans(trans)  # head->mri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BIDS vs MNE-Python Coordinate Systems\n\nBIDS has many acceptable coordinate systems for iEEG, which can be viewed in\n`appendix VIII`_ of the BIDS specification.\nHowever, MNE-BIDS depends on MNE-Python and MNE-Python does not support all\nthese coordinate systems (yet).\n\nMNE-Python has a few tutorials on this topic:\n\n- `background on FreeSurfer`_\n- `MNE-Python coordinate frames`_\n\nMNE-Python supports using ``mni_tal`` and ``mri`` coordinate frames,\ncorresponding to the ``fsaverage`` and ``ACPC`` (for an ACPC-aligned T1) BIDS\ncoordinate systems respectively. All other coordinate coordinate frames in\nMNE-Python, if written with :func:`mne_bids.write_raw_bids`, must have\nan :attr:`mne_bids.BIDSPath.space` specified, and will be read in with\nthe montage channel locations set to the coordinate frame 'unknown'.\n\n## Step 2: Formatting as BIDS\n\nNow, let us format the `Raw` object into BIDS.\n\nWith this step, we have everything to start a new BIDS directory using\nour data. To do that, we can use :func:`write_raw_bids`\nGenerally, :func:`write_raw_bids` tries to extract as much\nmeta data as possible from the raw data and then formats it in a BIDS\ncompatible way. :func:`write_raw_bids` takes a bunch of inputs, most of\nwhich are however optional. The required inputs are:\n\n- :code:`raw`\n- :code:`bids_basename`\n- :code:`bids_root`\n\n... as you can see in the docstring:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(write_raw_bids.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us initialize some of the necessary data for the subject.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# There is a subject, and specific task for the dataset.\nsubject_id = '1'\ntask = 'motor'\n\n# get MNE-Python directory w/ example data\nmne_data_dir = mne.get_config('MNE_DATASETS_MISC_PATH')\n\n# There is the root directory for where we will write our data.\nbids_root = op.join(mne_data_dir, 'ieeg_bids')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To ensure the output path doesn't contain any leftover files from previous\ntests and example runs, we simply delete it.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>Do not delete directories that may contain important data!</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if op.exists(bids_root):\n    shutil.rmtree(bids_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we just need make a :class:`mne_bids.BIDSPath` to save the data.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>By passing ``acpc_aligned=True``, we are affirming that\n             the T1 in this dataset is aligned to ACPC. This is very\n             difficult to check with a computer which is why this\n             step is required.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Now convert our data to be in a new BIDS dataset.\nbids_path = BIDSPath(subject=subject_id, task=task, root=bids_root)\n\n# plot T1 to show that it is ACPC-aligned\n# note that the origin is centered on the anterior commissure (AC)\n# with the y-axis passing through the posterior commissure (PC)\nT1_fname = op.join(subjects_dir, 'sample_seeg', 'mri', 'T1.mgz')\nfig = plot_anat(T1_fname, cut_coords=(0, 0, 0))\nfig.axes['x'].ax.annotate('AC', (2., -2.), (30., -40.), color='w',\n                          arrowprops=dict(facecolor='w', alpha=0.5))\nfig.axes['x'].ax.annotate('PC', (-31., -2.), (-80., -40.), color='w',\n                          arrowprops=dict(facecolor='w', alpha=0.5))\n\n# write ACPC-aligned T1\nlandmarks = get_anat_landmarks(T1_fname, raw.info, trans,\n                               'sample_seeg', subjects_dir)\nT1_bids_path = write_anat(T1_fname, bids_path, deface=True,\n                          landmarks=landmarks)\n\n# write `raw` to BIDS and anonymize it (converts to BrainVision format)\n#\n# we need to pass the `montage` argument for coordinate frames other than\n# \"head\" which is what MNE uses internally in the `raw` object\n#\n# `acpc_aligned=True` affirms that our MRI is aligned to ACPC\n# if this is not true, convert to `fsaverage` (see below)!\nwrite_raw_bids(raw, bids_path, anonymize=dict(daysback=40000),\n               montage=montage, acpc_aligned=True, overwrite=True)\n\n# check our output\nprint_dir_tree(bids_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MNE-BIDS has created a suitable directory structure for us, and among other\nmeta data files, it started an ``events.tsv`` and ``channels.tsv`` file,\nand created an initial ``dataset_description.json`` file on top!\n\nNow it's time to manually check the BIDS directory and the meta files to add\nall the information that MNE-BIDS could not infer. For instance, you must\ndescribe ``iEEGReference`` and ``iEEGGround`` yourself.\nIt's easy to find these by searching for ``\"n/a\"`` in the sidecar files.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search_folder_for_text('n/a', bids_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember that there is a convenient JavaScript tool to validate all your BIDS\ndirectories called the \"BIDS-validator\", available as a web version and a\ncommand line tool:\n\nWeb version: https://bids-standard.github.io/bids-validator/\n\nCommand line tool: https://www.npmjs.com/package/bids-validator\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load channels from BIDS-formatted dataset and compare\n\nNow we have written our BIDS directory. We can use\n:func:`read_raw_bids` to read in the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# read in the BIDS dataset to plot the coordinates\nraw2 = read_raw_bids(bids_path=bids_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have to go back to \"head\" coordinates with the head->mri transform.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>If you were downloading this from ``OpenNeuro``, you would\n          have to run the Freesurfer ``recon-all`` to get the transforms.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "montage2 = raw2.get_montage()\n\n# this uses Freesurfer recon-all subject directory\nmontage2.add_estimated_fiducials('sample_seeg', subjects_dir=subjects_dir)\n\n# get head->mri trans, invert from mri->head\ntrans2 = mne.transforms.invert_transform(\n    mne.channels.compute_native_head_t(montage2))\n\n# now the montage is properly in \"head\" and ready for analysis in MNE\nraw2.set_montage(montage2)\n\n# get the monage, apply the trans and make sure it's the same\n# note: the head coordinates may differ because they are defined by\n# the fiducials which are estimated; as long as the head->mri trans\n# is computed with the same fiducials, the coordinates will be the same\n# in ACPC space which is what matters\nmontage2 = raw.get_montage()  # get montage in 'head' coordinates\nmontage2.apply_trans(trans2)\n\n# compare with standard\nprint('Recovered coordinate: {recovered}\\n'\n      'Saved coordinate:     {saved}'.format(\n          recovered=montage2.get_positions()['ch_pos']['LENT 1'],\n          saved=montage.get_positions()['ch_pos']['LENT 1']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Cite mne-bids\nWe can see that the appropriate citations are already written in the README.\nIf you are preparing a manuscript, please make sure to also cite MNE-BIDS\nthere.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "readme = op.join(bids_root, 'README')\nwith open(readme, 'r', encoding='utf-8-sig') as fid:\n    text = fid.read()\nprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Store coordinates in a template space\nAlternatively, if your T1 is not aligned to ACPC-space or you prefer to\nstore the coordinates in a template space (e.g. ``fsaverage``) for another\nreason, you can also do that.\n\nHere we'll use the MNI Talairach transform to get to ``fsaverage`` space\nfrom \"mri\" aka surface RAS space.\n``fsaverage`` is very useful for group analysis as shown in\n`tut-working-with-seeg`. Note, this is only a linear transform and so\none loses quite a bit of accuracy relative to the needs of intracranial\nresearchers so it is quite suboptimal. A better option is to use a\nsymmetric diffeomorphic transform to create a one-to-one mapping of brain\nvoxels from the individual's brain to the template as shown in\n`tut-ieeg-localize`. Even so, it's better to provide the coordinates\nin the individual's brain space, as was done above, so that the researcher\nwho uses the coordinates has the ability to tranform them to a template\nof their choice.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ensure the output path doesn't contain any leftover files from previous\n# tests and example runs\nif op.exists(bids_root):\n    shutil.rmtree(bids_root)\n\n# load our raw data again\nraw = mne.io.read_raw_fif(op.join(\n    misc_path, 'seeg', 'sample_seeg_ieeg.fif'))\nraw.info['line_freq'] = 60  # specify power line frequency as required by BIDS\n\n# get Talairach transform\nmri_mni_t = mne.read_talxfm('sample_seeg', subjects_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's convert the montage to MNI Talairach (\"mni_tal\").\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "montage = raw.get_montage()\nmontage.apply_trans(trans)  # head->mri\nmontage.apply_trans(mri_mni_t)\n\n# write to BIDS, this time with a template coordinate system\nwrite_raw_bids(raw, bids_path, anonymize=dict(daysback=40000),\n               montage=montage, overwrite=True)\n\n# read in the BIDS dataset\nraw2 = read_raw_bids(bids_path=bids_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we should go back to \"head\" coordinates. We do this with ``fsaverage``\nfiducials which are in MNI space. In this case, you would not need to run\nthe Freesurfer ``recon-all`` for the subject, you would just need a\n``subjects_dir`` with ``fsaverage`` in it, which is accessible using\n:func:`mne.datasets.fetch_fsaverage`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "montage2 = raw2.get_montage()\n# add fiducials for \"mni_tal\" (which is the coordinate frame fsaverage is in)\n# so that it can properly be set to \"head\"\nmontage2.add_mni_fiducials(subjects_dir=subjects_dir)\n\n# get the new head->mri (in this case mri == mni because fsavearge is in MNI)\nmni_head_t = mne.channels.compute_native_head_t(montage2)\n\n# set the montage transforming to the \"head\" coordinate frame\nraw2.set_montage(montage2)\n\n# check that we can recover the coordinates\nprint('Recovered coordinate head: {recovered}\\n'\n      'Saved coordinate head:     {saved}'.format(\n          recovered=raw2.info['chs'][0]['loc'][:3],\n          saved=raw.info['chs'][0]['loc'][:3]))\n\n# check difference in trans\nprint('Recovered trans:\\n{recovered}\\n'\n      'Original trans:\\n{saved}'.format(\n          recovered=mni_head_t['trans'].round(3),\n          # combine head->mri with mri->mni to get head->mni\n          # and then invert to get mni->head\n          saved=np.linalg.inv(np.dot(trans['trans'], mri_mni_t['trans'])\n                              ).round(3)))\n\n# ensure that the data in MNI coordinates is exactly the same\n# (within numerical precision)\nmontage2 = raw2.get_montage()  # get montage after transformed back to head\nmontage2.apply_trans(mne.transforms.invert_transform(mni_head_t))\nprint('Recovered coordinate: {recovered}\\n'\n      'Saved coordinate:     {saved}'.format(\n          recovered=montage2.get_positions()['ch_pos']['LENT 1'],\n          saved=montage.get_positions()['ch_pos']['LENT 1']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see the coordinates stored in the ``raw`` object are slightly off.\nThis is because the ``head`` coordinate frame is defined by the fiducials\n(nasion, left and right pre-auricular points), and, in the first case,\nthe fiducials were found on the individual anatomy and then transformed\nto MNI space, whereas, in the second case, they were found directly on\nthe template brain (this was done once for the template so that we could\njust load it from a file). This difference means that there are slightly\ndifferent head->mri transforms. Once these transforms are applied, however,\nthe positions are the same in MNI coordinates which is what is important.\n\nAs a final step, let's go over how to assign the fiducials for a template\nbrain where they are not found for you. Many template coordinate systems\nare allowed by BIDS but are not used in MNE-Python.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>As of this writing, BIDS accepts channel coordinates in reference to the\n    the following template spaces: ``ICBM452AirSpace``,\n    ``ICBM452Warp5Space``, ``IXI549Space``, ``fsaverage``, ``fsaverageSym``,\n    ``fsLR``, ``MNIColin27``, ``MNI152Lin``,\n    ``MNI152NLin2009[a-c][Sym|Asym]``, ``MNI152NLin6Sym``,\n    ``MNI152NLin6ASym``, ``MNI305``, ``NIHPD``, ``OASIS30AntsOASISAnts``,\n    ``OASIS30Atropos``, ``Talairach`` and ``UNCInfant``. As discussed above,\n    it is recommended to share the coordinates in the individual subject's\n    anatomical reference frame so that researchers who use the data can\n    transform the coordinates to any of these templates that they choose.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pos = montage.get_positions()\n# fiducial points are included in the sample data but could be found using\n# Freeview (note the coordinates are in MNE \"mri\" coordinates which is the\n# same as Freesurfers TkRegRAS but in meters not millimeters --\n# divide the TkRegRAS values by 1000)\n# or fiducial points could also be found with the MNE coregistration GUI\nnas = pos['nasion']\nlpa = pos['lpa']\nrpa = pos['rpa']\n\nprint('Fiducial points determined from the template head anatomy:\\n'\n      f'nasion: {nas}\\nlpa:    {lpa}\\nrpa:    {rpa}')\n\n# read raw in again to start over\nraw2 = read_raw_bids(bids_path=bids_path)\nmontage2 = raw2.get_montage()\n\n# note: for fsaverage, the montage will be in the coordinate frame\n# 'mni_tal' because it is recognized by MNE but other templates will be\n# in the 'unknown' coordinate frame because they are not recognized by MNE\npos2 = montage2.get_positions()\n\n# here we will set the coordinate frame to be 'mri' because our channel\n# positions and fiducials are in the Freesurfer surface RAS coordinate\n# frame of the template T1 MRI (in this case fsaverage)\nmontage2 = mne.channels.make_dig_montage(  # add fiducials\n    ch_pos=pos2['ch_pos'],\n    nasion=nas,\n    lpa=lpa,\n    rpa=rpa,\n    coord_frame='mri')\n\n# get head->mri trans, invert from mri->head\ntrans2 = mne.transforms.invert_transform(\n    mne.channels.compute_native_head_t(montage2))\n\n# set the montage to transform back to 'head'\nraw2.set_montage(montage2)\n\n# check that the coordinates were recovered\nmontage2 = raw2.get_montage()  # get montage after transformed back to head\nmontage2.apply_trans(trans2)\nprint('Recovered coordinate: {recovered}\\n'\n      'Saved coordinate:     {saved}'.format(\n          recovered=montage2.get_positions()['ch_pos']['LENT 1'],\n          saved=montage.get_positions()['ch_pos']['LENT 1']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We showed how to add the fiducials from ``montage.add_mni_fiducials``\nyourself, which allows us to use any coordinate frame, even if it's not MNI,\nbut we still assumed that the coordinates were in Freesurfer surface RAS.\nUnfortunately, this is not guranteed to be the case because the BIDS\ntemplate descriptions only specify the anatomical space (as defined by the\ntemplate T1 MRI) not the coordinate frame of the space; the coordinates\ncould be in terms of surface RAS, voxels of the template MRI or scanner RAS\nMRI coordinates. See `tut-source-alignment` for a tutorial explaining\nthe different coordinate frames. If the coordinates are in voxels or scanner\nRAS, we'll have to find the fiducials in those same coordinates.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# get the template T1 (must be mgz to have surface RAS transforms\n# the Freesurfer command `mri_convert` can be used to convert\nsubjects_dir = op.join(mne.datasets.sample.data_path(), 'subjects')\ntemplate_T1 = nib.load(op.join(subjects_dir, 'fsaverage', 'mri', 'T1.mgz'))\n\n# get vox->mri transform\nvox_mri_t = template_T1.header.get_vox2ras_tkr()\n\n# transform the channel data to voxels just to demonstrate how to transform it\n# back (this is the case where the BIDS-formatted data in the template space is\n# in voxel coordinates so it would already be transformed when read in)\nraw = mne.io.read_raw_fif(op.join(  # load our raw data again\n    misc_path, 'seeg', 'sample_seeg_ieeg.fif'))\nmontage = raw.get_montage()  # get the original montage\nmontage.apply_trans(trans)  # head->mri\nmri_vox_trans = mne.transforms.Transform(\n    fro='mri',\n    to='mri_voxel',\n    trans=np.linalg.inv(vox_mri_t)\n)\nscale_t = np.eye(4)\nscale_t[:3, :3] *= 1000  # m->mm\nmontage.apply_trans(mne.transforms.Transform('mri', 'mri', scale_t))\nmontage.apply_trans(mri_vox_trans)  # transform our original montage\n\n# print transformed fiducials, these could be found in the voxel locator in\n# Freesurfer's `freeview` based on the template MRI since we wouldn't have\n# them found for us for a template other than fsaverage\npos = montage.get_positions()\nnas = pos['nasion']\nlpa = pos['lpa']\nrpa = pos['rpa']\n\nprint('Fiducial points determined from the template head anatomy in voxels:\\n'\n      f'nasion: {nas}\\nlpa:    {lpa}\\nrpa:    {rpa}')\n\n# read raw in again to start over\nraw2 = read_raw_bids(bids_path=bids_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, it's the same as if we just got the montage from the raw in voxels\ni.e. if we would do ``montage = raw.get_montage()`` and get the positions\ndirectly in voxels instead of doing the transform to voxels first\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "montage2 = mne.channels.make_dig_montage(  # add fiducials\n    ch_pos=pos['ch_pos'],\n    nasion=nas,\n    lpa=lpa,\n    rpa=rpa,\n    coord_frame='mri_voxel')\nvox_mri_trans = mne.transforms.Transform(\n    fro='mri_voxel',\n    to='mri',\n    trans=vox_mri_t\n)\nmontage2.apply_trans(vox_mri_trans)\nscale_t = np.eye(4)\nscale_t[:3, :3] /= 1000  # mm->m\nmontage2.apply_trans(mne.transforms.Transform('mri', 'mri', scale_t))\n\n# get head->mri trans, invert from mri->head\ntrans2 = mne.transforms.invert_transform(\n    mne.channels.compute_native_head_t(montage2))\n\n# set the montage to transform back to 'head'\nraw2.set_montage(montage2)\n\n# check that the coordinates were recovered\nmontage = raw.get_montage()  # get the original montage\nmontage.apply_trans(trans)  # head->mri\nmontage2 = raw2.get_montage()  # get montage after transformed back to head\nmontage2.apply_trans(trans2)\nprint('Recovered coordinate: {recovered}\\n'\n      'Saved coordinate:     {saved}'.format(\n          recovered=montage2.get_positions()['ch_pos']['LENT 1'],\n          saved=montage.get_positions()['ch_pos']['LENT 1']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, the template could also be in scanner RAS:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# transform the channel data to scanner RAS just to demonstrate how to\n# transform it back (this is the case where the BIDS-formatted data in the\n# template space is in scanner RAS coordinates so it would already be\n# transformed when read in)\nraw = mne.io.read_raw_fif(op.join(  # load our raw data again\n    misc_path, 'seeg', 'sample_seeg_ieeg.fif'))\nmontage = raw.get_montage()  # get the original montage\nmontage.apply_trans(trans)  # head->mri\nscale_t = np.eye(4)\nscale_t[:3, :3] *= 1000  # m->mm\nmontage.apply_trans(mne.transforms.Transform('mri', 'mri', scale_t))\nmontage.apply_trans(mri_vox_trans)  # transform our original montage\nvox_ras_t = template_T1.header.get_vox2ras()  # no tkr for scanner RAS\nvox_ras_trans = mne.transforms.Transform(\n    fro='mri_voxel',\n    to='ras',\n    trans=vox_ras_t\n)\nmontage.apply_trans(vox_ras_trans)\n\n# print transformed fiducials, these could be found in the RAS locator in\n# Freesurfer's `freeview` based on the template MRI\n# note in this case, they are in mm and should not be transformed\npos = montage.get_positions()\nnas = pos['nasion']\nlpa = pos['lpa']\nrpa = pos['rpa']\n\nprint('Fiducial points determined from the template head anatomy in '\n      f'scanner RAS:\\nnasion: {nas}\\nlpa:    {lpa}\\nrpa:    {rpa}')\n\n# read raw in again to start over\nraw2 = read_raw_bids(bids_path=bids_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, it's the same as if we just got the montage from the raw in scanner RAS\ni.e. we would use ``montage = raw.get_montage()`` instead of having to do\nthe transforms above\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "montage2 = mne.channels.make_dig_montage(  # add fiducials\n    ch_pos=pos['ch_pos'],\n    nasion=nas,\n    lpa=lpa,\n    rpa=rpa,\n    coord_frame='ras')\nras_vox_t = template_T1.header.get_ras2vox()\nvox_mri_t = template_T1.header.get_vox2ras_tkr()\nras_mri_trans = mne.transforms.Transform(\n    fro='ras',\n    to='mri',\n    trans=np.dot(ras_vox_t, vox_mri_t)  # combine ras->vox and vox->mri to get\n)                                       # ras->mri\nmontage2.apply_trans(ras_mri_trans)\nscale_t = np.eye(4)\nscale_t[:3, :3] /= 1000  # mm->m\nmontage2.apply_trans(mne.transforms.Transform('mri', 'mri', scale_t))\n\n# get head->mri trans, invert from mri->head\ntrans2 = mne.transforms.invert_transform(\n    mne.channels.compute_native_head_t(montage2))\n\n# set the montage to transform back to 'head'\nraw2.set_montage(montage2)\n\n# check that the coordinates were recovered exactly this time\nmontage = raw.get_montage()  # get the original montage\nmontage.apply_trans(trans)  # head->mri\nmontage2 = raw2.get_montage()  # get montage after transformed back to head\nmontage2.apply_trans(trans2)\nprint('Recovered coordinate: {recovered}\\n'\n      'Saved coordinate:     {saved}'.format(\n          recovered=montage2.get_positions()['ch_pos']['LENT 1'],\n          saved=montage.get_positions()['ch_pos']['LENT 1']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In summary, as we saw, these standard template spaces that are allowable by\nBIDS are quite complicated. We therefore only cover these cases because\ndatasets are allowed to be in these coordinate systems, and we want to be\nable to analyze them with MNE-Python. The template coordinate spaces\ndon't specify a coordinate frame, so it is better to save the raw data in\nthe individual's ACPC space, allowing the person analyzing the data to\ntransform the positions to whatever template they want. Thus, we recommend\nif at all possible, saving BIDS iEEG data in ACPC coordinate space\ncorresponding to the individual subject's brain, not in a template\ncoordinate frame.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}